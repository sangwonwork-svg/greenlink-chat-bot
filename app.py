import streamlit as st
from langchain_groq import ChatGroq
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings

# ê²½ë¡œë¥¼ ë” ëª…í™•í•˜ê²Œ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤.
from langchain.chains.retrieval import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate

# --- 1. ë³´ì•ˆ ì„¤ì • ---
GROQ_API_KEY = st.secrets["GROQ_API_KEY"]
ACCESS_PASSWORD = st.secrets["ACCESS_PASSWORD"]

st.set_page_config(page_title="ì‚¬ë‚´ ë§¤ë‰´ì–¼ ì±—ë´‡", layout="centered")

# --- 2. ë¡œê·¸ì¸ ê¸°ëŠ¥ ---
if "auth" not in st.session_state:
    st.session_state.auth = False

if not st.session_state.auth:
    st.title("ğŸ”’ ì ‘ê·¼ ì œí•œ")
    pwd = st.text_input("ì ‘ì† ë¹„ë°€ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”:", type="password")
    if st.button("ë¡œê·¸ì¸"):
        if pwd == ACCESS_PASSWORD:
            st.session_state.auth = True
            st.rerun()
        else:
            st.error("ë¹„ë°€ë²ˆí˜¸ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    st.stop()

# --- 3. ë§¤ë‰´ì–¼ í•™ìŠµ (ìºì‹±) ---
@st.cache_resource
def init_rag():
    loader = PyPDFLoader("manual.pdf")
    pages = loader.load_and_split()
    embeddings = HuggingFaceEmbeddings(model_name="jhgan/ko-sroberta-multitask")
    vectorstore = FAISS.from_documents(pages, embeddings)
    return vectorstore

try:
    vector_db = init_rag()
except Exception as e:
    st.error(f"ë§¤ë‰´ì–¼ ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    st.stop()

# --- 4. ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ ---
st.title("ğŸ¤– ì‚¬ë‚´ ë§¤ë‰´ì–¼ ì–´ì‹œìŠ¤í„´íŠ¸")

if "messages" not in st.session_state:
    st.session_state.messages = []

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        llm = ChatGroq(
            groq_api_key=GROQ_API_KEY,
            model_name="llama-3.3-70b-versatile",
            temperature=0
        )
        
        # ìµœì‹  ë°©ì‹ì˜ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        system_prompt = (
            "ë„ˆëŠ” íšŒì‚¬ì˜ ë§¤ë‰´ì–¼ ì „ë¬¸ê°€ì•¼. "
            "ì•„ë˜ ì œê³µëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì¤˜. "
            "ë§¤ë‰´ì–¼ì— ì—†ëŠ” ë‚´ìš©ì´ë¼ë©´ 'ì£„ì†¡í•˜ì§€ë§Œ í•´ë‹¹ ë‚´ìš©ì€ ë§¤ë‰´ì–¼ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'ë¼ê³  ë‹µí•´ì¤˜."
            "\n\n"
            "{context}"
        )
        
        prompt_template = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("human", "{input}"),
        ])
        
        # ìµœì‹  ë°©ì‹ì˜ RAG ì²´ì¸ ìƒì„±
        question_answer_chain = create_stuff_documents_chain(llm, prompt_template)
        rag_chain = create_retrieval_chain(vector_db.as_retriever(), question_answer_chain)
        
        with st.spinner("ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
            response = rag_chain.invoke({"input": prompt})
            answer = response["answer"]
            st.markdown(answer)
            st.session_state.messages.append({"role": "assistant", "content": answer})
