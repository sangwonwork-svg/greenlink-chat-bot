import streamlit as st
from langchain_groq import ChatGroq
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings # í•œêµ­ì–´ ìµœì í™”ìš©
from langchain.chains import RetrievalQA

# --- 1. í™˜ê²½ ì„¤ì • ---
GROQ_API_KEY = "ì—¬ê¸°ì—_Groq_API_í‚¤ë¥¼_ì…ë ¥í•˜ì„¸ìš”"
ACCESS_PASSWORD = "ìš°ë¦¬ë¼ë¦¬ë¹„ë²ˆ" # ë‹´ë‹¹ì 3ëª…ë§Œ ê³µìœ í•  ë¹„ë²ˆ

st.set_page_config(page_title="ì‚¬ë‚´ ë§¤ë‰´ì–¼ ì±—ë´‡", layout="centered")

# --- 2. ë¡œê·¸ì¸ ì²´í¬ ---
if "auth" not in st.session_state:
    st.session_state.auth = False

if not st.session_state.auth:
    pwd = st.text_input("ì ‘ì† ë¹„ë°€ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”:", type="password")
    if pwd == ACCESS_PASSWORD:
        st.session_state.auth = True
        st.rerun()
    else:
        if pwd: st.error("ë¹„ë°€ë²ˆí˜¸ê°€ í‹€ë ¸ìŠµë‹ˆë‹¤.")
        st.stop()

# --- 3. ë°ì´í„° ë¡œë”© (ìºì‹± ì²˜ë¦¬ë¡œ ì†ë„ í–¥ìƒ) ---
@st.cache_resource
def load_manual():
    # PDF ë¡œë“œ (ë§¤ë‰´ì–¼ íŒŒì¼ëª…ì„ 'manual.pdf'ë¡œ í•´ì„œ ê°™ì€ í´ë”ì— ë‘ì„¸ìš”)
    loader = PyPDFLoader("manual.pdf")
    pages = loader.load_and_split()
    
    # í•œêµ­ì–´ ì„±ëŠ¥ì´ ì¢‹ì€ ë¬´ë£Œ ì„ë² ë”© ëª¨ë¸
    embeddings = HuggingFaceEmbeddings(model_name="jhgan/ko-sroberta-multitask")
    
    # ë²¡í„° DB ìƒì„±
    vectorstore = FAISS.from_documents(pages, embeddings)
    return vectorstore

vector_db = load_manual()

# --- 4. ì±—ë´‡ UI ---
st.title("ğŸ“„ ì‚¬ë‚´ ë§¤ë‰´ì–¼ ì§€ì‹ê³ ")
st.caption("ë‹´ë‹¹ì ì „ìš© ë§¤ë‰´ì–¼ ê²€ìƒ‰ ì„œë¹„ìŠ¤ (ëª¨ë°”ì¼ ì§€ì›)")

if "messages" not in st.session_state:
    st.session_state.messages = []

# ëŒ€í™” ë‚´ì—­ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì§ˆë¬¸ ì…ë ¥
if prompt := st.chat_input("ë§¤ë‰´ì–¼ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        # Groqì˜ Llama 3.3 70B ëª¨ë¸ í˜¸ì¶œ
        llm = ChatGroq(
            groq_api_key=GROQ_API_KEY,
            model_name="llama-3.3-70b-versatile",
            temperature=0
        )
        
        # RAG ì²´ì¸ ìƒì„±
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=vector_db.as_retriever()
        )
        
        # ë‹µë³€ ìƒì„± (í•œêµ­ì–´ ê°•ì¡° í”„ë¡¬í”„íŠ¸ í¬í•¨)
        response = qa_chain.invoke(f"ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì¤˜. ë§¤ë‰´ì–¼ì— ì—†ëŠ” ë‚´ìš©ì€ ëª¨ë¥¸ë‹¤ê³  ë§í•´ì¤˜. ì§ˆë¬¸: {prompt}")
        answer = response["result"]
        
        st.markdown(answer)
        st.session_state.messages.append({"role": "assistant", "content": answer})
