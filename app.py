import streamlit as st
from langchain_groq import ChatGroq
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.chains.retrieval_qa.base import RetrievalQA
# ë˜ëŠ” ë‹¨ìˆœí•˜ê²Œ ì•„ë˜ì²˜ëŸ¼ ìœ ì§€í•˜ë˜ requirementsê°€ ì •ìƒ ì„¤ì¹˜ë˜ë©´ í•´ê²°ë©ë‹ˆë‹¤.

# --- 1. ë³´ì•ˆ ì„¤ì • (Streamlit Secretsì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°) ---
# ë¡œì»¬ í…ŒìŠ¤íŠ¸ ì‹œì—ëŠ” .streamlit/secrets.toml íŒŒì¼ì„ ë§Œë“¤ì–´ ì €ì¥í•˜ì„¸ìš”.
GROQ_API_KEY = st.secrets["GROQ_API_KEY"]
ACCESS_PASSWORD = st.secrets["ACCESS_PASSWORD"]

st.set_page_config(page_title="ì‚¬ë‚´ ë§¤ë‰´ì–¼ ì±—ë´‡", layout="centered")

# --- 2. ê°„ë‹¨ ë¡œê·¸ì¸ ê¸°ëŠ¥ ---
if "auth" not in st.session_state:
    st.session_state.auth = False

if not st.session_state.auth:
    st.title("ğŸ”’ ì ‘ê·¼ ì œí•œ")
    pwd = st.text_input("ì ‘ì† ë¹„ë°€ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”:", type="password")
    if st.button("ë¡œê·¸ì¸"):
        if pwd == ACCESS_PASSWORD:
            st.session_state.auth = True
            st.rerun()
        else:
            st.error("ë¹„ë°€ë²ˆí˜¸ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    st.stop()

# --- 3. ë§¤ë‰´ì–¼ í•™ìŠµ (ìºì‹±) ---
@st.cache_resource
def init_rag():
    # manual.pdf íŒŒì¼ì´ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
    loader = PyPDFLoader("manual.pdf")
    pages = loader.load_and_split()
    
    # í•œêµ­ì–´ ë¬¸ì¥ ìœ ì‚¬ë„ ì¸¡ì •ì— íŠ¹í™”ëœ ëª¨ë¸
    embeddings = HuggingFaceEmbeddings(model_name="jhgan/ko-sroberta-multitask")
    
    vectorstore = FAISS.from_documents(pages, embeddings)
    return vectorstore

try:
    vector_db = init_rag()
except Exception as e:
    st.error(f"ë§¤ë‰´ì–¼ ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    st.stop()

# --- 4. ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ ---
st.title("ğŸ¤– ì‚¬ë‚´ ë§¤ë‰´ì–¼ ì–´ì‹œìŠ¤í„´íŠ¸")
st.info("ë§¤ë‰´ì–¼ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤. ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")

if "messages" not in st.session_state:
    st.session_state.messages = []

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        llm = ChatGroq(
            groq_api_key=GROQ_API_KEY,
            model_name="llama-3.3-70b-versatile",
            temperature=0
        )
        
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=vector_db.as_retriever(search_kwargs={"k": 3})
        )
        
        # í•œêµ­ì–´ ë‹µë³€ ìœ ë„ë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        sys_prompt = f"ë„ˆëŠ” íšŒì‚¬ì˜ ë§¤ë‰´ì–¼ ì „ë¬¸ê°€ì•¼. ì œê³µëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì¤˜. ë§¤ë‰´ì–¼ì— ì—†ëŠ” ë‚´ìš©ì´ë¼ë©´ 'ì£„ì†¡í•˜ì§€ë§Œ í•´ë‹¹ ë‚´ìš©ì€ ë§¤ë‰´ì–¼ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'ë¼ê³  ë‹µí•´ì¤˜. ì§ˆë¬¸: {prompt}"
        
        with st.spinner("ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
            response = qa_chain.invoke(sys_prompt)
            answer = response["result"]
            st.markdown(answer)
            st.session_state.messages.append({"role": "assistant", "content": answer})
